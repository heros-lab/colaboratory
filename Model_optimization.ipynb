{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heros-lab/colaboratory/blob/master/Model_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhJLFfAg-Mvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "!pip install optuna\n",
        "import optuna\n",
        "from optuna.integration import KerasPruningCallback\n",
        "\n",
        "work_path = \"/content/drive/My Drive/Colab Notebooks\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4MWBcF_85AM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class optimize_manager:\n",
        "    def __init__(self, max_units, study_name):\n",
        "        self.max_units = max_units\n",
        "        self.score_path  = f\"{work_path}/score_{study_name}.csv\"\n",
        "        self.result_path = f\"{work_path}/result_{study_name}.csv\"\n",
        "\n",
        "    def filtering_with_IQR(self, data_list):\n",
        "        pd_series = pd.Series(data_list)\n",
        "        q1 = pd_series.quantile(.25)\n",
        "        q3 = pd_series.quantile(.75)\n",
        "        iqr = q3 - q1\n",
        "        lim_upper = q3 + iqr*1.5\n",
        "        lim_lower = q1 - iqr*1.5\n",
        "        return pd_series[pd_series.apply(lambda x:lim_lower < x < lim_upper)]    \n",
        "\n",
        "    def save_scores(self, count, units, data_list):\n",
        "        with open(self.score_path, \"w\" if count == 0 else \"a\") as file:\n",
        "            file.write(f\"#{count}\")\n",
        "            for num_unit in units:\n",
        "                file.write(f\", {num_unit}\")\n",
        "            for data in data_list:\n",
        "                file.write(f\", {data:.6e}\")\n",
        "            file.write(\"\\n\")\n",
        "\n",
        "    def save_results(self, trial_id, units, samples, mean, std, mean_f, std_f):\n",
        "        if trial_id == 0:\n",
        "            mode = \"w\"\n",
        "            header = \"Trials\"\n",
        "            for i in range(len(units)):\n",
        "                header += f\", Unit-{i+1}\"\n",
        "            header += \", Samples(Full:101), Estimated loss, Standard-deviation, Estimated loss(filter), Standard-deviation(filter)\\n\"\n",
        "        else:\n",
        "            mode = \"a\"\n",
        "            header = \"\"\n",
        "\n",
        "        with open(self.result_path, mode) as file:\n",
        "            file.write(header)\n",
        "            file.write(f\"#{trial_id}\")\n",
        "            for num_unit in units:\n",
        "                file.write(f\", {num_unit}\")\n",
        "            file.write(f\", {samples}, {mean:.6e}, {std:.6e}, {mean_f:.6e}, {std_f:.6e}\\n\")\n",
        "\n",
        "    def objective(self, trial):\n",
        "        epochs = 200\n",
        "        num_batch = 512\n",
        "        num_sample = 101\n",
        "        num_units  = [trial.suggest_int(f\"num_unit{i+1}\", 1, self.max_units[i]) for i in range(len(self.max_units))]\n",
        "\n",
        "        score_list = []\n",
        "        for i in range(num_sample):\n",
        "            clear_session()\n",
        "            print(f\"\\r#{trial.number:2} -- unit: {num_units}, sampling: {i+1}/{num_sample}\", end=\"\")\n",
        "            \n",
        "            model = Sequential()\n",
        "            model.add(Dense(\n",
        "                    input_dim=learn_x.shape[1], units=num_units[0],\n",
        "                    activation=\"tanh\", kernel_initializer=\"glorot_uniform\"))\n",
        "            for i in range(len(num_units) - 1):\n",
        "                model.add(Dense(\n",
        "                    input_dim=num_units[i], units=num_units[i+1],\n",
        "                    activation=\"tanh\", kernel_initializer=\"glorot_uniform\"))\n",
        "            model.add(Dense(input_dim=num_units[-1], units=1))\n",
        "            model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
        "            model.fit(learn_x, learn_y, batch_size=num_batch, epochs=epochs, verbose=0)\n",
        "\n",
        "            score = model.evaluate(test_x, test_y, batch_size=test_x.shape[0], verbose=0)\n",
        "            score_list.append(score)\n",
        "        \n",
        "        # フィルタ処理\n",
        "        score_list_flt = self.filtering_with_IQR(score_list)\n",
        "        \n",
        "        # 平均と標準偏差の算出\n",
        "        mean, std = pd.Series(score_list).describe().loc[[\"mean\",\"std\"]]        \n",
        "        samples, mean_f, std_f = score_list_flt.describe().loc[[\"count\",\"mean\",\"std\"]]\n",
        "        \n",
        "        # 保存＆結果出力\n",
        "        self.save_scores(trial.number, num_units, score_list)\n",
        "        self.save_results(trial.number, num_units, samples, mean, std, mean_f, std_f)\n",
        "        print(f\"\\r#{trial.number:2} -- unit: {num_units}, samples: {samples}/101, mean: {mean:.4e}, std: {std:.4e}\")\n",
        "\n",
        "        return mean"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLZiBtuaZBIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_name = \"ms3a\"\n",
        "test_name  = \"ms2a\"\n",
        "\n",
        "df_learn_x = pd.read_csv(f\"{work_path}/data/{learn_name}_x.csv\")\n",
        "df_learn_y = pd.read_csv(f\"{work_path}/data/{learn_name}_y.csv\")\n",
        "\n",
        "df_test_x = pd.read_csv(f\"{work_path}/data/{test_name}_x.csv\")\n",
        "df_test_y = pd.read_csv(f\"{work_path}/data/{test_name}_y.csv\")\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLnWVIbO_J40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_index = [0,1,2,3,4,5,6]   # index for conv.\n",
        "#x_index = [0,3,4,6]    # index for prop.1\n",
        "#x_index = [1,2,5,6]    # index for prop.2\n",
        "#x_index = [1,2,5,6]    # index for prop.3\n",
        "#x_index = [0,3,4,5,6]  # index for prop.4\n",
        "y_index = [1]\n",
        "\n",
        "learn_x = df_learn_x.values[:, x_index]\n",
        "learn_y = df_learn_y.values[:, y_index]\n",
        "test_x = df_test_x.values[:, x_index]\n",
        "test_y = df_test_y.values[:, y_index]\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZCEHBSEvRbr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c362a78-516f-4fbc-b955-4f9d7f3436a9"
      },
      "source": [
        "tag = \"conv2\"\n",
        "ver = \"0.2\"\n",
        "max_units = [20, 20]\n",
        "\n",
        "STUDY_LOADING = False\n",
        "\n",
        "storage_path = f\"sqlite:///{work_path}/opt_model_{tag}.db\"\n",
        "study_name = tag + \"_ver\" + ver\n",
        "manager = optimize_manager(max_units, study_name)\n",
        "\n",
        "\n",
        "# study load or create\n",
        "if STUDY_LOADING:\n",
        "    study = optuna.load_study(study_name, storage_path, pruner=optuna.pruners.MedianPruner())\n",
        "else:\n",
        "    study = optuna.create_study(study_name=study_name, storage=storage_path, direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[I 2020-06-20 11:10:46,582] A new study created with name: conv2_ver0.2\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHoh1z78I6z2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "528ba7c7-3a87-45cc-b032-c33625d48630"
      },
      "source": [
        "study.optimize(manager.objective, n_trials=10)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"*** All Trial are finished!! ***\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# 0 -- unit: [18, 20], sampling: 16/101"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DMPC0VYa8sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}